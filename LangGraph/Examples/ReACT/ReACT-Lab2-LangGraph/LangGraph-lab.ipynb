{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-16T20:15:44.252149Z",
     "start_time": "2025-06-16T20:15:44.249224Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "# ensure the environment variable is set\n",
    "print(\"OPENAI_API_KEY is loaded from .env file:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"TAVILY_API_KEY is loaded from .env file:\", os.getenv(\"TAVILY_API_KEY\"))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is loaded from .env file: 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\n",
      "TAVILY_API_KEY is loaded from .env file: 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\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T20:15:46.425990Z",
     "start_time": "2025-06-16T20:15:46.423595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# install LangGraph and langchain\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "## Search engine we are going to use as a tool for our Lab2\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ],
   "id": "76ca8c8c2cd07ca9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T19:56:13.686819Z",
     "start_time": "2025-06-16T19:56:13.684046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We create the instance of the Tavily Search tool\n",
    "# It will return top 4 results from Search API (Top-K = 4)\n",
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "\n",
    "print(tool.name)\n",
    "# it will print: 'tavily_search_results_json'\n",
    "# this is what Language Model will use to call the Tavily Search API"
   ],
   "id": "fa27bc889c9d50a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
      "tavily_search_results_json\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T19:56:55.331550Z",
     "start_time": "2025-06-16T19:56:55.329169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent State definition\n",
    "# right now it's just an annotated list of messages where we will add to over time\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ],
   "id": "ce798646c643bf17",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# here we create the agent\n",
    "# it has 3 methods:\n",
    "# 1. to call the OpenAI\n",
    "# 2. to check if such action exists\n",
    "# 3. to execute the action\n",
    "class Agent:\n",
    "    # we specify what tool to use, what model, and the system message\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        # we store the system message as an attribute of Agent class\n",
    "        self.system = system\n",
    "        # we initialize the graph\n",
    "        # right now it has no nodes or edges attached\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        # conditional edge which declares what method to call in each case\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            # if it's TRUE it goes to action node, if False - 'END' node\n",
    "            # END node is a special node in LangGraph which indicates the end of the graph\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        # regular edge which connects action and llm nodes\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        # set the entry point of the graph\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        # compile the graph to turn it into chain runnable by LangGraph\n",
    "        self.graph = graph.compile()\n",
    "        # dictionary of the tools, we save the name of the tool to the tool itself\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        # on the model we bind the tools to the model\n",
    "        # we pass the list of the tools to the model property calling bind_tools on the model passing the tools name list\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    # all methods at the edges takes the state as an argument\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ],
   "id": "c56e5754889194ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
